{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**READING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read the data\n",
        "df = pd.read_csv('all_calls_2022-20234.csv')\n",
        "mobility= pd.read_csv('Mobility.csv', delimiter = ';')\n",
        "pop_dens= pd.read_csv('pop_dens.csv', delimiter = ';')\n",
        "cars= pd.read_csv('Traffic.csv', delimiter = ';')\n",
        "\n",
        "#Read holiday data  for 2022, 2023 and 2024\n",
        "holidays_2022 = pd.read_csv('Holidays_2022.csv')\n",
        "holidays_2023 = pd.read_csv('Holidays_2023.csv')\n",
        "holidays_2024 = pd.read_csv('Holidays_2024.csv')\n",
        "\n",
        "dict_1= {'PV22':'Drenthe', 'PV23':'Overijssel', 'PV24':'Flevoland', 'PV25':'Gelderland', 'PV26':'Utrecht','PV27':'Noord-Holland', 'PV28':'Zuid-Holland','PV30':'Noord-Brabant', 'PV31':'Limburg'}\n",
        "dict_1\n",
        "\n",
        "dict_2= {\n",
        " 'Alkmaar': 'Noord-Holland',\n",
        " 'Almere': 'Flevoland',\n",
        " 'Amersfoort': 'Utrecht',\n",
        " 'Amstelveen': 'Noord-Holland',\n",
        " 'Amsterdam': 'Noord-Holland',\n",
        " 'Arnhem': 'Gelderland',\n",
        " 'Assen': 'Drenthe',\n",
        " 'Breda': 'Noord-Brabant',\n",
        " 'Bussum': 'Noord-Holland',\n",
        " 'Deventer': 'Overijssel',\n",
        " 'Delft': 'Zuid-Holland',\n",
        " 'Den Haag': 'Zuid-Holland',\n",
        " 'Den Helder': 'Noord-Holland',\n",
        " 'Eindhoven': 'Noord-Brabant',\n",
        " 'Hengelo': 'Overijssel',\n",
        " 'Hoofddorp': 'Noord-Holland',\n",
        " 'Hoorn': 'Noord-Holland',\n",
        " 'IJsselstein': 'Utrecht',\n",
        " 'Katwijk': 'Zuid-Holland',\n",
        " 'Leiden': 'Zuid-Holland',\n",
        " 'Maastricht': 'Limburg',\n",
        " 'Nijmegen': 'Gelderland',\n",
        " 'Nootdorp': 'Zuid-Holland',\n",
        " 'Oosterhout': 'Noord-Brabant',\n",
        " 'Pijnacker': 'Zuid-Holland',\n",
        " 'Purmerend': 'Noord-Holland',\n",
        " 'Roermond': 'Limburg',\n",
        " 'Rotterdam': 'Zuid-Holland',\n",
        " 'Tilburg': 'Noord-Brabant',\n",
        " 'Utrecht': 'Utrecht',\n",
        " 'Valkenburg': 'Limburg',\n",
        " 'Veenendaal': 'Utrecht',\n",
        " 'Veldhoven': 'Noord-Brabant',\n",
        " 'Venray': 'Limburg',\n",
        " 'Weert': 'Limburg',\n",
        " 'Zaandam': 'Noord-Holland'\n",
        "}\n",
        "\n",
        "# Map 'City' to 'region_names' using 'dict_2'\n",
        "df['region_names'] = df['City'].map(dict_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLEANING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop rows with missing region names\n",
        "df = df.dropna(subset=['region_names'])\n",
        "pop_dens['Perioden'].unique()\n",
        "#replace 2022JJ00 with 2023JJ00\n",
        "pop_dens['Perioden'] = pop_dens['Perioden'].replace('2022JJ00', '2023JJ00')\n",
        "# population data \n",
        "pop_dens['RegioS'] = pop_dens['RegioS'].str.strip()\n",
        "pop_dens['region_names'] = pop_dens['RegioS'].map(dict_1)\n",
        "\n",
        "# cars data\n",
        "cars['RegioS'] = cars['RegioS'].str.strip() \n",
        "cars['region_names'] = cars['RegioS'].map(dict_1)\n",
        "\n",
        "cars_recent = cars[cars['Perioden'] == '2023JJ00']\n",
        "\n",
        "\n",
        "cars_recent = cars_recent.drop('ID',axis=1) \n",
        "cars_recent = cars_recent.drop('RegioS',axis=1) \n",
        "cars_recent.rename(columns={'TotaalPersonenautoS_3': 'cars_per_region'}, inplace=True) \n",
        "cars_recent.rename(columns={'PersonenautoSRelatief_4': 'cars_per_region_1000'}, inplace=True)\n",
        "mobility['RegionCharacteristics'] = mobility['RegionCharacteristics'].str.strip() #remove whitespace from the column\n",
        "\n",
        "# Map 'RegionCharacteristics' to 'region_names' using 'dict_1'\n",
        "mobility['region_names'] = mobility['RegionCharacteristics'].map(dict_1)\n",
        "#replace 2022JJ00 with 2023JJ00\n",
        "mobility['Periods'] = mobility['Periods'].replace('2022JJ00', '2023JJ00')\n",
        "\n",
        "#subset only 2023 data\n",
        "mobility = mobility[mobility['Periods'] == '2023JJ00']\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['ID', 'TravelMotives', 'Population', 'Margins',\n",
        "                   'RegionCharacteristics', 'Periods', 'TravelModes',\n",
        "                   'Trips_1', 'Trips_4', 'DistanceTravelled_2',\n",
        "                   'TimeTravelled_3', 'TimeTravelled_6']\n",
        "mobility.drop(columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Clean up 'DistanceTravelled_5' and convert commas to periods\n",
        "mobility['DistanceTravelled_5'] = mobility['DistanceTravelled_5'].str.strip().replace(',', '.')\n",
        "\n",
        "# Convert 'DistanceTravelled_5' to numeric, coercing errors to NaN\n",
        "pd.to_numeric(mobility['DistanceTravelled_5'], errors='coerce')\n",
        "\n",
        "print(mobility['region_names'].duplicated().sum())\n",
        "print(cars_recent['region_names'].duplicated().sum())\n",
        "print(pop_dens['region_names'].duplicated().sum())\n",
        "#keep the first occurence of the duplicated region_names\n",
        "mobility = mobility.drop_duplicates(subset=['region_names'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merge with population density\n",
        "df2 = df.merge(pop_dens, on ='region_names' , how= 'left')\n",
        " # use formula to merge 3 dfs at the same time for the same columns\n",
        "#merge with mobility\n",
        "df3 = df2.merge(mobility, on ='region_names' , how='left')\n",
        "#merge with cars\n",
        "df = df3.merge(cars_recent, on ='region_names' , how= 'inner')\n",
        "#drop RegioS_x and Perioden_x columns\n",
        "df.drop(['Perioden_x', 'Perioden_y','Geslacht'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MERGING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#rename Bevolkingsdichtheig to Population_Dens\n",
        "df.rename(columns={'Bevolkingsdichtheid_2':'Population_Dens','region_names':'Region'}, inplace=True)\n",
        "#Create new Date column from DateTime column in df\n",
        "df['Date'] = pd.to_datetime(df['DateTime']).dt.date\n",
        "\n",
        "#transform Datum column in holidays_2022, holidays_2023 and holidays_2024 to Date \n",
        "holidays_2022['Date'] = pd.to_datetime(holidays_2022['Datum']).dt.date\n",
        "\n",
        "# Combine the dfs\n",
        "holidays_all_years = pd.concat([holidays_2022, holidays_2023, holidays_2024])\n",
        "\n",
        "# Convert the date column to datetime\n",
        "holidays_all_years['Datum'] = pd.to_datetime(holidays_all_years['Date'])\n",
        "\n",
        "#Merge the dataframes in new holiday column\n",
        "df = df.merge(holidays_all_years, on=['Date'], how='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CLEANING DATA SOME MORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Convert 'ConversationTime' to timedelta and then to seconds, coercing errors\n",
        "df['ConversationTime_Seconds'] = pd.to_timedelta(df['ConversationTime'], errors='coerce').dt.total_seconds()\n",
        "df['WaitTime_Seconds'] = pd.to_timedelta(df['WaitTime'], errors='coerce').dt.total_seconds()\n",
        "\n",
        "# Check for NaN values after conversion and fill w median\n",
        "median_conversation_time = df['ConversationTime_Seconds'].median()\n",
        "df['ConversationTime_Seconds'].fillna(median_conversation_time, inplace=True)\n",
        "\n",
        "# Convert 'DateTime' to DateTime, coercing errors, this will turn the invalid dates to NaT and Datetime will contain only valid dates\n",
        "df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')\n",
        "\n",
        "\n",
        "#exteract day of week from datetime\n",
        "df['Day_of_Week'] = df['DateTime'].dt.day_name()\n",
        "#extract hour of day\n",
        "df['Hour_of_Day'] = df['DateTime'].dt.hour\n",
        "#extract week\n",
        "df['Week'] = df['DateTime'].dt.isocalendar().week\n",
        "#extract year\n",
        "df['Year'] = df['DateTime'].dt.year\n",
        "#find NaTs in Waittime\n",
        "df[df['WaitTime'].isnull()]\n",
        "\n",
        "df['Facility'].unique()\n",
        "\n",
        "\n",
        "# we need to remove rows with these facilities from facility column\n",
        "df = df[~df['Facility'].isin(['Tilburg Pieter Vredeplein', 'Tilburg Emma passage', 'Tilburg Koningsplein','Leiden Garenmarkt','Tilburg Tivoli', 'Leiden Haarlemmerstraat', 'Tilburg Zwijssen','Tilburg Stappegoor','Tilburg Schouwburgplein', 'Leiden Soestdijkkade', 'Leiden Lammermarkt','Leiden Morspoort'])]\n",
        "#check for uniques in operator column\n",
        "df['Operator'].unique()\n",
        "\n",
        "#drop rows with operators that are not representative of the operations in operator column\n",
        "df = df[~df['Operator'].isin(['Administrator', 'Meldkamer Tilburg', 'flex','m.elvery','test','-'])] # ~ means not\n",
        "\n",
        "#anonimize operator column by encoding to Operator 1, Operator 2, Operator 3 in new column\n",
        "df['Operator_ID'] = df['Operator'].astype('category').cat.codes\n",
        "#replace operator -1 with nan\n",
        "df['Operator_ID'] = df['Operator_ID'].replace(-1, np.nan)\n",
        "#drop nan rows\n",
        "df = df.dropna(subset=['Operator_ID'])\n",
        "\n",
        "# Display the first few rows to confirm the changes\n",
        "df['Operator'].head()\n",
        "# Create a separate DataFrame with operator ID and operator name without duplicates\n",
        "operator_df = df[['Operator_ID', 'Operator', 'Desk']].drop_duplicates().sort_values(by='Operator_ID')\n",
        "\n",
        "# Replace -1 with NaN in the 'Operator_ID' column\n",
        "operator_df['Operator_ID'] = operator_df['Operator_ID'].replace(-1, np.nan)\n",
        "# turn nan values in flag column to '0'\n",
        "df['Flag_BarrierOpened'] = df['Flag_BarrierOpened'].fillna(0)\n",
        "# Check for '-' entries and turn them to NaN\n",
        "df = df.replace('-', np.nan)\n",
        "\n",
        "# Check for null values in the dataset\n",
        "df.isnull().sum()\n",
        "#drop type column\n",
        "df.drop(['Type'], axis=1, inplace=True)\n",
        "\n",
        "#create new column for holidays\n",
        "df['Holiday'] = np.where(df['Datum'].isnull(), 0, 1)\n",
        "\n",
        "#drop Datum and feestdag column\n",
        "df.drop(['Datum'], axis=1, inplace=True)\n",
        "df.drop(['Feestdag'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#check the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print all column names\n",
        "for col in df.columns:\n",
        "    print(col)\n",
        "\n",
        "#find date range of the data\n",
        "print(df['Date'].min())\n",
        "print(df['Date'].max())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['City'].unique()\n",
        "#drop cities we dont need\n",
        "df = df[~df['City'].isin(['Arnhem', 'Almere'])]\n",
        "#find rows where city is nan\n",
        "df[df['City'].isnull()]\n",
        "\n",
        "#there are 100 rows where city is nan, we can drop them\n",
        "df = df.dropna(subset=['City'])\n",
        "df['Reason'].unique()\n",
        "\n",
        "#count values in reason\n",
        "df['Reason'].value_counts()\n",
        "\n",
        "#operationally, Sometimes operators forget to click a reason for each call, especiallly during peak moments. Which is why there is a high number of calls with no reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check for null values in desk column\n",
        "df[df['Desk'].isnull()]\n",
        "\n",
        "#remove rows with null values in desk column\n",
        "df = df.dropna(subset=['Desk'])\n",
        "#split the device column \n",
        "df['Device_Copy'] = df['Device'].copy() \n",
        "df['Last_Split'] = df['Device_Copy'].str.rsplit('/', n=1) \n",
        "\n",
        "\n",
        "# Expand the result into separate columns \n",
        "df[['Before_Last', 'Last']] = pd.DataFrame(df['Last_Split'].tolist(), index=df.index) \n",
        "\n",
        "# Drop the intermediate 'Last_Split' column \n",
        "df = df.drop(['Device', 'Last_Split'], axis=1) \n",
        "\n",
        "#Rename the columns \n",
        "df.rename(columns={'Before_Last':'City/Facility', 'Last':'Device'}, inplace=True) \n",
        "\n",
        "\n",
        "#drop device copy and city/facility columns  \n",
        "df = df.drop(['Device_Copy'], axis=1) \n",
        "df = df.drop(['City/Facility'], axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate IQR for 'WaitTime_Seconds'\n",
        "Q1_wait = df['WaitTime_Seconds'].quantile(0.25)\n",
        "Q3_wait = df['WaitTime_Seconds'].quantile(0.75)\n",
        "IQR_wait = Q3_wait - Q1_wait\n",
        "\n",
        "# Filter out data points with 'WaitTime_Seconds' exceeding the IQR threshold\n",
        "f_df = df[(df['WaitTime_Seconds'] >= (Q1_wait - 1.5 * IQR_wait)) & (df['WaitTime_Seconds'] <= (Q3_wait + 1.5 * IQR_wait))]\n",
        "\n",
        "#  calculate and filter 'ConversationTime_Seconds'\n",
        "Q1_conversation = df['ConversationTime_Seconds'].quantile(0.25)\n",
        "Q3_conversation = df['ConversationTime_Seconds'].quantile(0.75)\n",
        "IQR_conversation = Q3_conversation - Q1_conversation\n",
        "\n",
        "f_df = f_df[(f_df['ConversationTime_Seconds'] >= (Q1_conversation - 1.5 * IQR_conversation)) & (f_df['ConversationTime_Seconds'] <= (Q3_conversation + 1.5 * IQR_conversation))]\n",
        "\n",
        "# Now 'f_df' contains your DataFrame with outliers removed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#replace nan values in flag column with 0\n",
        "f_df['Flag_BarrierOpened'] = f_df['Flag_BarrierOpened'].fillna(0)\n",
        "#convert data types\n",
        "f_df['Operator_ID'] = f_df['Operator_ID'].astype(int)\n",
        "f_df['Holiday'] = f_df['Holiday'].astype(int)\n",
        "#f_df['Population_Dens'] = f_df['Population_Dens'].astype(int)\n",
        "\n",
        "f_df['City'] = f_df['City'].astype('category')\n",
        "f_df['Facility'] = f_df['Facility'].astype('category')\n",
        "f_df['Reason'] = f_df['Reason'].astype('string')\n",
        "#f_df['Region'] = f_df['Region'].astype('category')\n",
        "f_df['Day_of_Week'] = f_df['Day_of_Week'].astype('category')\n",
        "f_df['Hour_of_Day'] = f_df['Hour_of_Day'].astype('category')\n",
        "f_df['Device'] = f_df['Device'].astype('string')\n",
        "f_df['Flag_BarrierOpened'] = f_df['Flag_BarrierOpened'].astype(int)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#boxplots to verify outliers removal\n",
        "sns.boxplot(x=f_df['WaitTime_Seconds'])\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(x=f_df['ConversationTime_Seconds'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#CHECK distribution of WaitTime_Seconds\n",
        "sns.displot(f_df['WaitTime_Seconds'], kde=True, bins=20)\n",
        "#annotate median\n",
        "plt.axvline(f_df['WaitTime_Seconds'].median(), color='r', linestyle='dashed', linewidth=1)\n",
        "#annotate mean\n",
        "plt.axvline(f_df['WaitTime_Seconds'].mean(), color='g', linestyle='dashed', linewidth=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create calls per day column\n",
        "f_df['Calls_per_day'] = f_df.groupby('Date')['Date'].transform('count')\n",
        "\n",
        "#calculate calls per day per facility\n",
        "calls_per_day_facility = f_df.groupby(['Facility', 'Date'])['Facility'].count().reset_index(name='CallCount')\n",
        "\n",
        "# Display the top three facilities with the highest average calls per day \n",
        "top_facilities = calls_per_day_facility.groupby('Facility')['CallCount'].mean().sort_values(ascending=False).head(5)\n",
        "print(top_facilities)\n",
        "\n",
        "# Group by Operator_ID and Hour_of_Day, then calculate the average wait time\n",
        "avg_wait_per_hour_operator = f_df.groupby(['Operator_ID', 'Hour_of_Day'])['WaitTime_Seconds'].mean().reset_index(name='AverageWaitTime')\n",
        "\n",
        "# Calculate the number of days\n",
        "num_days = f_df['Date'].nunique()\n",
        "\n",
        "# Calculate the average wait time per hour for each operator\n",
        "# The number of days is used to average over different days\n",
        "avg_wait_per_hour_operator['AverageWaitTimePerDay'] = avg_wait_per_hour_operator['AverageWaitTime'] / num_days\n",
        "avg_wait_per_hour_operator = avg_wait_per_hour_operator[['Operator_ID', 'Hour_of_Day', 'AverageWaitTimePerDay']]\n",
        "\n",
        "# Now, avg_wait_per_hour_operator contains the average wait time per day for each hour for each operator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#visualize calls per day per facility\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='Facility', y='Calls_per_day', data=f_df)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title('Calls per day per facility')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        " # Loop through each facility and plot its daily call count \n",
        "\n",
        " #for facility in calls_per_day_facility['Facility'].unique(): \n",
        "\n",
        "#    # subset = calls_per_day_facility[calls_per_day_facility['Facility'] == facility] \n",
        "#     plt.figure(figsize=(15, 5)) \n",
        "#     plt.plot(pd.to_datetime(subset['Date']), subset['CallCount'], label=facility) \n",
        "#     plt.title(f'Call Count per Day for {facility}') \n",
        "#     plt.xlabel('Date') \n",
        "#     plt.ylabel('Number of Calls')\n",
        "#     plt.legend() \n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eda\n",
        "\n",
        "#check count of reported issues per city\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(x='City', data=f_df, order=f_df['City'].value_counts().index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "#more eda, check count of reported issues per facility, ordered by count on the x axis\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "#plot count on x axis\n",
        "sns.countplot(y='Facility', data=f_df, order=f_df['Facility'].value_counts().index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "#check count of reported issues per day of week\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(y='Day_of_Week', data=f_df, order=order, palette='Reds')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Number of Calls')\n",
        "plt.ylabel('Day of Week')\n",
        "plt.show()\n",
        "\n",
        "#more eda, AVG wait time per day of week, ordered monday to sunday\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(y='Day_of_Week', x='WaitTime_Seconds', data=f_df, order=order, palette='Blues')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Average Wait Time (Seconds)')\n",
        "plt.ylabel('Day of Week')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#more eda\n",
        "#check count of reported issues per hour of day\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(x='Hour_of_Day', data=f_df,palette='Reds')\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#more eda, AVG wait time per hour of day , ordered starting from 7am to 7 am\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(x='Hour_of_Day', y='WaitTime_Seconds', data=f_df, palette='Blues')\n",
        "#yticks as seconds\n",
        "plt.yticks(np.arange(0, 12, 1))\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Average Wait Time (Seconds)')\n",
        "plt.show()\n",
        "\n",
        "#selecting a model\n",
        "#check the distribution of the target variable\n",
        "\n",
        "#check distribution of WaitTime_Seconds\n",
        "sns.displot(f_df['WaitTime_Seconds'], kde=True, bins=20)\n",
        "#annotate median\n",
        "plt.axvline(f_df['WaitTime_Seconds'].median(), color='r', linestyle='dashed', linewidth=1)\n",
        "#annotate mean\n",
        "plt.axvline(f_df['WaitTime_Seconds'].mean(), color='g', linestyle='dashed', linewidth=1)\n",
        "plt.show()\n",
        "\n",
        "#check distribution of ConversationTime_Seconds\n",
        "sns.displot(f_df['ConversationTime_Seconds'], kde=True, bins=20)\n",
        "#annotate median\n",
        "plt.axvline(f_df['ConversationTime_Seconds'].median(), color='r', linestyle='dashed', linewidth=1)\n",
        "#annotate mean\n",
        "plt.axvline(f_df['ConversationTime_Seconds'].mean(), color='g', linestyle='dashed', linewidth=1)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#more target variables could be : \n",
        "#- wait time per day of week\n",
        "#- wait time per hour of day\n",
        "#- wait time per facility\n",
        "#- wait time per city\n",
        "#- wait time per operator\n",
        "#- wait time per reason\n",
        "#- wait time per device\n",
        "\n",
        "# or we could predict the number of calls per day per facility by using the date as a target variable\n",
        "#for example we could predict the number of calls per day for the next 30 days for each facility\n",
        "\n",
        "#check distribution of calls per day per facility\n",
        "sns.displot(calls_per_day_facility['CallCount'], kde=True, bins=20)\n",
        "#annotate median\n",
        "plt.axvline(calls_per_day_facility['CallCount'].median(), color='r', linestyle='dashed', linewidth=1)\n",
        "#annotate mean\n",
        "plt.axvline(calls_per_day_facility['CallCount'].mean(), color='g', linestyle='dashed', linewidth=1)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Created flag column for the devices  \n",
        "f_df['Device_1'] = f_df['Device'].astype(str) \n",
        "\n",
        "\n",
        "def check_exit(value): \n",
        "    keywords = ['UIT', 'Exit', 'Uitrit'] \n",
        "    return any(keyword in value for keyword in keywords) \n",
        "\n",
        "def check_entry(value): \n",
        "    keywords = ['IN', 'Entry', 'ENTREE', 'Inrit'] \n",
        "    return any(keyword in value for keyword in keywords) \n",
        "\n",
        "def check_atm(value): \n",
        "    keywords = ['BA', 'Kassa'] # looks for keyword in string value\n",
        "    return any(keyword in value for keyword in keywords) \n",
        "\n",
        "def check_gate(value): \n",
        "    keywords = ['SG', 'GATE'] \n",
        "    return any(keyword in value for keyword in keywords) \n",
        "\n",
        "def check_door(value): \n",
        "    keywords = ['Door', 'Deur', 'DL'] \n",
        "    return any(keyword in value for keyword in keywords) \n",
        "\n",
        "# Apply the functions \n",
        "f_df['Exit'] = f_df['Device_1'].apply(check_exit).astype(int) \n",
        "f_df['Entry'] = f_df['Device_1'].apply(check_entry).astype(int) \n",
        "f_df['Pay'] = f_df['Device_1'].apply(check_atm).astype(int) \n",
        "f_df['Gate'] = f_df['Device_1'].apply(check_gate).astype(int) \n",
        "f_df['Door'] = f_df['Device_1'].apply(check_door).astype(int) \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "#turn NA to No reason entered\n",
        "f_df['Reason'] = f_df['Reason'].fillna('No reason entered')\n",
        "\n",
        "\n",
        "##Seasonal trends for when its summer and winter \n",
        "f_df['IsSummer'] = f_df['DateTime'].dt.month.isin([6, 7, 8]).astype(int)\n",
        "f_df['IsSpring'] = f_df['DateTime'].dt.month.isin([3, 4, 5]).astype(int)\n",
        "f_df['IsAutumn'] = f_df['DateTime'].dt.month.isin([9, 10, 11]).astype(int)\n",
        "f_df['IsWinter'] = f_df['DateTime'].dt.month.isin([12, 1, 2]).astype(int)\n",
        "# Group by year and week and count the number of calls\n",
        "weekly_call_counts = f_df.groupby(['Year', 'Week']).size().reset_index(name='CallCountPerWeek')\n",
        "\n",
        "# Display the first few rows\n",
        "weekly_call_counts.head()\n",
        "\n",
        "#merge with f_df\n",
        "f_df = f_df.merge(weekly_call_counts, on=['Year', 'Week'], how='left')\n",
        "\n",
        "#created a column Is_weekend to find out if calls were made in the weekends \n",
        "## Map day names to weekend or not \n",
        "weekend_mapping = {'Monday': False, 'Tuesday': False, 'Wednesday': False, 'Thursday': False, 'Friday': False, 'Saturday': True, 'Sunday': True} \n",
        "f_df['IsWeekend'] = f_df['Day_of_Week'].map(weekend_mapping) \n",
        "\n",
        "#turn nan to 0\n",
        "f_df['IsWeekend'] = f_df['IsWeekend'].fillna(0)\n",
        "\n",
        "\n",
        "#drop nulls in conversation time\n",
        "f_df = f_df.dropna(subset=['ConversationTime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate mean calls per hour\n",
        "mean_calls_per_hour = f_df.groupby('Hour_of_Day').size().mean()\n",
        "\n",
        "# Identify busy hours by comparing each hour's call count with the mean\n",
        "busy_hours = f_df.groupby('Hour_of_Day').size()[f_df.groupby('Hour_of_Day').size() > mean_calls_per_hour].index\n",
        "\n",
        "# Create a binary feature for busy hour\n",
        "f_df['IsBusyHour'] = f_df['Hour_of_Day'].apply(lambda x: 1 if x in busy_hours else 0)\n",
        "\n",
        "# For lagged features, let's say we want a 1-hour lag\n",
        "f_df['LaggedBusyHour'] = f_df['IsBusyHour'].shift(1).fillna(method='bfill')\n",
        "\n",
        "# Create a lagged column for the call count.\n",
        "# We'll use a 1-hour lag again\n",
        "f_df['LaggedCallCount'] = f_df['CallCountPerWeek'].shift(1).fillna(method='bfill')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check if is busy hour makes sense \n",
        "f_df[f_df['IsBusyHour'] == 1]['Hour_of_Day'].value_counts()\n",
        "\n",
        "#it makes sense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the median\n",
        "median_calls_per_day = f_df['Calls_per_day'].median()\n",
        "median_call_count_per_week = f_df['CallCountPerWeek'].median()\n",
        "\n",
        "# Bin the Calls_per_day column\n",
        "f_df['Calls_per_day_Bin'] = pd.cut(f_df['Calls_per_day'], \n",
        "                                    bins=[-np.inf, median_calls_per_day, np.inf], \n",
        "                                    labels=['Low_Call_Day', 'High_Call_Day'])\n",
        "\n",
        "# Bin the CallCountPerWeek column\n",
        "f_df['CallCountPerWeek_Bin'] = pd.cut(f_df['CallCountPerWeek'], \n",
        "                                       bins=[-np.inf, median_call_count_per_week, np.inf], \n",
        "                                       labels=['Low_Call_Week', 'High_Call_Week'])\n",
        "\n",
        "# Create flags for Calls_per_day\n",
        "f_df['Low_Call_Day_Flag'] = np.where(f_df['Calls_per_day_Bin'] == 'Low_Call_Day', 1, 0)\n",
        "f_df['High_Call_Day_Flag'] = np.where(f_df['Calls_per_day_Bin'] == 'High_Call_Day', 1, 0)\n",
        "\n",
        "# Create flags for CallCountPerWeek\n",
        "f_df['Low_Call_Week_Flag'] = np.where(f_df['CallCountPerWeek_Bin'] == 'Low_Call_Week', 1, 0)\n",
        "f_df['High_Call_Week_Flag'] = np.where(f_df['CallCountPerWeek_Bin'] == 'High_Call_Week', 1, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the ratio features\n",
        "small_value = 0.001\n",
        "f_df['CallDurationRatio'] = f_df['WaitTime_Seconds'] / (f_df['ConversationTime_Seconds'] + small_value)\n",
        "f_df['PopulationCarRatio'] = f_df['Population_Dens'] / (f_df['cars_per_region'] + small_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "#turn nan to -\n",
        "f_df['ReasonEnteredByUser'] = f_df['ReasonEnteredByUser'].fillna('-')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check what days are the ones most often when it is a high call week, but low call day\n",
        "f_df[(f_df['High_Call_Week_Flag'] == 1) & (f_df['Low_Call_Day_Flag'] == 1)]['Day_of_Week'].value_counts()\n",
        "\n",
        "#visualize with days of week\n",
        "order= ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(y='Day_of_Week', data=f_df[(f_df['High_Call_Week_Flag'] == 1) & (f_df['Low_Call_Day_Flag'] == 1)], order=order, palette='Reds')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n",
        "#check what days are the ones most often when it is a low call week, but high call day\n",
        "f_df[(f_df['Low_Call_Week_Flag'] == 1) & (f_df['High_Call_Day_Flag'] == 1)]['Day_of_Week'].value_counts()\n",
        "\n",
        "#visualize with days of week\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(y='Day_of_Week', data=f_df[(f_df['Low_Call_Week_Flag'] == 1) & (f_df['High_Call_Day_Flag'] == 1)], order=order, palette='Blues')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "#drop ReasonEnteredByUser and Operator columns\n",
        "f_df = f_df.drop(['ReasonEnteredByUser'], axis=1)\n",
        "f_df = f_df.drop(['Operator'], axis=1)\n",
        "\n",
        "#create month column\n",
        "f_df['Month'] = f_df['DateTime'].dt.month\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check total nr of times each device was used ine ach facility\n",
        "device_usage_per_facility = f_df.groupby('Facility').agg({\n",
        "    'Gate': 'sum',\n",
        "    'Entry': 'sum',\n",
        "    'Exit': 'sum',\n",
        "    'Pay': 'sum',\n",
        "    'Door': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "\n",
        "# Display the first few rows\n",
        "device_usage_per_facility.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "barrier_opened_count = f_df.groupby('Facility')['Flag_BarrierOpened'].sum().reset_index(name='BarrierOpenedCount')\n",
        "device_usage_df = device_usage_per_facility.merge(barrier_opened_count, on='Facility', how='left')\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 16))\n",
        "sns.barplot(x='BarrierOpenedCount', y='Facility', data=device_usage_df, color='red')\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=90)\n",
        "# Set labels and title\n",
        "plt.xlabel('Facility')\n",
        "plt.ylabel('Barrier Opened Count')\n",
        "plt.title('Barrier Opened Count per Facility')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "#merge with f_df\n",
        "f_df = f_df.merge(device_usage_df, on='Facility', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "f_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#drop gate_y, entry_y, exit_y, pay_y, door_y columns\n",
        "f_df = f_df.drop(['Gate_y', 'Entry_y', 'Exit_y', 'Pay_y', 'Door_y'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_df['BarrierOpened_DayOfWeek_Interaction'] = f_df['Flag_BarrierOpened'] * f_df['DateTime'].dt.dayofweek # interaction between barrier opened and day of week\n",
        "f_df['BarrierOpened_HourOfDay_Interaction'] = f_df['Flag_BarrierOpened'] * f_df['DateTime'].dt.hour # interaction between barrier opened and hour of day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "#encode categorical variables\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "#encode facility\n",
        "f_df['Facility'] = labelencoder.fit_transform(f_df['Facility'])\n",
        "\n",
        "#encode city\n",
        "f_df['City'] = labelencoder.fit_transform(f_df['City'])\n",
        "\n",
        "#encode region\n",
        "f_df['Region'] = labelencoder.fit_transform(f_df['Region'])\n",
        "\n",
        "#encode day of week\n",
        "f_df['Day_of_Week'] = labelencoder.fit_transform(f_df['Day_of_Week'])\n",
        "\n",
        "#encode device\n",
        "f_df['Device'] = labelencoder.fit_transform(f_df['Device'])\n",
        "\n",
        "#encode operator\n",
        "f_df['Operator_ID'] = labelencoder.fit_transform(f_df['Operator_ID'])\n",
        "\n",
        "#encode reason\n",
        "f_df['Reason'] = labelencoder.fit_transform(f_df['Reason'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#drop regioS\n",
        "f_df = f_df.drop(['RegioS'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check the data\n",
        "f_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check the data\n",
        "f_df.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot of calls per week\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.lineplot(x='Date', y='CallCountPerWeek', data=f_df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#plot calls_per_day\n",
        "plt.figure(figsize=(15, 5))\n",
        "sns.lineplot(x='Date', y='Calls_per_day', data=f_df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = f_df[['Population_Dens', 'cars_per_region', 'cars_per_region_1000', \n",
        "                    'ConversationTime_Seconds', 'WaitTime_Seconds', 'Hour_of_Day', \n",
        "                    'Operator_ID', 'Holiday', 'Exit_x', 'Entry_x', 'Pay_x', 'Gate_x', \n",
        "                    'Door_x', 'IsSummer', 'IsSpring', 'IsAutumn', 'IsWinter', \n",
        "                    'CallCountPerWeek', 'IsWeekend', 'IsBusyHour', 'LaggedBusyHour', 'Week', 'Year', 'Calls_per_day', 'CallDurationRatio', 'PopulationCarRatio','Month','BarrierOpenedCount']].corr()\n",
        "\n",
        "# Create a heatmap to visualize the correlation matrix\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "#encode calls per day bin and call per week bin\n",
        "f_df['Calls_per_day_Bin'] = labelencoder.fit_transform(f_df['Calls_per_day_Bin'])\n",
        "f_df['CallCountPerWeek_Bin'] = labelencoder.fit_transform(f_df['CallCountPerWeek_Bin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop DateTime and extract relevant datetime features separately\n",
        "f_df['Year'] = f_df['DateTime'].dt.year\n",
        "f_df['Month'] = f_df['DateTime'].dt.month\n",
        "f_df['Day'] = f_df['DateTime'].dt.day\n",
        "f_df['DayOfWeek'] = f_df['DateTime'].dt.dayofweek\n",
        "f_df['Hour'] = f_df['DateTime'].dt.hour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**MODEL DEVELOPMENT & SELECTION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#now we can select a model based on the distribution of the target variable\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate features and target variable\n",
        "X = f_df.drop(['DateTime', 'Desk', 'WaitTime', 'ConversationTime',\n",
        "       'Reason', 'Flag_MissedCall', 'Flag_NoCallReceived',\n",
        "       'ID',\n",
        "       'Date',\n",
        "       'Year', 'Operator_ID', 'Device',\n",
        "       'Calls_per_day', 'Device_1', 'Exit_x', 'Entry_x', 'Pay_x', 'Gate_x', 'Door_x', 'CallCountPerWeek', 'LaggedCallCount',\n",
        "       'Calls_per_day_Bin', 'CallCountPerWeek_Bin', 'Low_Call_Day_Flag', 'High_Call_Week_Flag','High_Call_Day_Flag','Week','Low_Call_Week_Flag','Month' \n",
        "], axis=1) #dropped month column, to avoid introducing biases or incorrect assumptions \n",
        "                 # Drop irrelevant columns\n",
        "y = f_df['High_Call_Week_Flag'] # Set target variable\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#decision tree classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train_scaled, y_train)\n",
        "y_pred = dt.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print('Accuracy score: ', accuracy_score(y_test, y_pred))\n",
        "print('Precision score: ', precision_score(y_test, y_pred))\n",
        "print('Recall score: ', recall_score(y_test, y_pred))\n",
        "print('F1 score: ', f1_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random forest classifier, predicting high call week flag\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate RF\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'ROC-AUC: {roc_auc}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random forest classifier, predicting high call week flag\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Random forest classifier, predicting high call week flag\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, max_features='sqrt', class_weight='balanced')\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 4: Evaluate Random Forest Classifier\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'ROC-AUC: {roc_auc}')\n",
        "\n",
        "# Step 5: Visualize classification results using confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test_scaled, y_test)\n",
        "plt.title('Confusion Matrix of Random Forest Classifier')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Reintroduce DateTime column to X from the original dataset f_df\n",
        "X['DateTime'] = f_df['DateTime']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random forest classifier, predicting high call week flag\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, ConfusionMatrixDisplay, classification_report, RocCurveDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Random forest classifier, predicting high call week flag\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, max_features='sqrt', class_weight='balanced')\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature Scaling (excluding DateTime column)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=['DateTime']))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=['DateTime']))\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 4: Evaluate Random Forest Classifier on Training Set\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_precision = precision_score(y_train, y_train_pred)\n",
        "train_recall = recall_score(y_train, y_train_pred)\n",
        "train_f1 = f1_score(y_train, y_train_pred)\n",
        "train_roc_auc = roc_auc_score(y_train, y_train_pred)\n",
        "\n",
        "print('--- Training Set Evaluation ---')\n",
        "print(f'Accuracy: {train_accuracy}')\n",
        "print(f'Precision: {train_precision}')\n",
        "print(f'Recall: {train_recall}')\n",
        "print(f'F1 Score: {train_f1}')\n",
        "print(f'ROC-AUC: {train_roc_auc}')\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "# Confusion Matrix for Training Set\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_train_scaled, y_train)\n",
        "plt.title('Confusion Matrix of Random Forest Classifier - Training Set')\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Evaluate Random Forest Classifier on Test Set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "print('--- Test Set Evaluation ---')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'ROC-AUC: {roc_auc}')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix for Test Set\n",
        "ConfusionMatrixDisplay.from_estimator(model, X_test_scaled, y_test)\n",
        "plt.title('Confusion Matrix of Random Forest Classifier - Test Set')\n",
        "plt.show()\n",
        "\n",
        "# Step 6: ROC Curve for Train and Test Sets\n",
        "RocCurveDisplay.from_estimator(model, X_train_scaled, y_train)\n",
        "plt.title('ROC Curve - Training Set')\n",
        "plt.show()\n",
        "\n",
        "RocCurveDisplay.from_estimator(model, X_test_scaled, y_test)\n",
        "plt.title('ROC Curve - Test Set')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a Logistic Regression Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "log_reg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 2: Splitting the Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Feature Scaling (excluding DateTime column)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train.drop(columns=['DateTime']))\n",
        "X_test_scaled = scaler.transform(X_test.drop(columns=['DateTime']))\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# Combine historical test predictions for continuity\n",
        "combined_dates = pd.to_datetime(X_test['DateTime'])\n",
        "combined_predictions = y_pred\n",
        "combined_actual = y_test\n",
        "\n",
        "# Aggregate weekly counts for historical data\n",
        "combined_df = pd.DataFrame({'Date': combined_dates, 'Predicted': combined_predictions, 'Actual': combined_actual})\n",
        "combined_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Resample to weekly data for clearer trend analysis\n",
        "weekly_agg = combined_df.resample('W').sum()\n",
        "# Evaluate Logistic Regression Model\n",
        "y_pred_log_reg = log_reg_model.predict(X_test_scaled)\n",
        "log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
        "log_reg_precision = precision_score(y_test, y_pred_log_reg)\n",
        "log_reg_recall = recall_score(y_test, y_pred_log_reg)\n",
        "log_reg_f1 = f1_score(y_test, y_pred_log_reg)\n",
        "log_reg_roc_auc = roc_auc_score(y_test, y_pred_log_reg)\n",
        "\n",
        "print(f'Logistic Regression - Accuracy: {log_reg_accuracy}')\n",
        "print(f'Logistic Regression - Precision: {log_reg_precision}')\n",
        "print(f'Logistic Regression - Recall: {log_reg_recall}')\n",
        "print(f'Logistic Regression - F1 Score: {log_reg_f1}')\n",
        "print(f'Logistic Regression - ROC-AUC: {log_reg_roc_auc}')\n",
        "\n",
        "# Confusion Matrix for Logistic Regression Model\n",
        "ConfusionMatrixDisplay.from_estimator(log_reg_model, X_test_scaled, y_test)\n",
        "plt.title('Confusion Matrix of Logistic Regression - Test Set')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve for Logistic Regression Model\n",
        "RocCurveDisplay.from_estimator(log_reg_model, X_test_scaled, y_test)\n",
        "plt.title('ROC Curve - Logistic Regression - Test Set')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "#addiotnal feature engineering\n",
        "# categorizing Hour of Day into business and non -business hours\n",
        "# business hours are from 9am to 5pm\n",
        "# non-business hours are from 5pm to 9am\n",
        "X['BusinessHour'] = X['Hour_of_Day'].apply(lambda x: 1 if 9 <= x <= 17 else 0)\n",
        "\n",
        "# adding morning, afternoong, evening & night\n",
        "\n",
        "X['Morning'] = X['Hour_of_Day'].apply(lambda x: 1 if 5 <= x <= 11 else 0)\n",
        "X['Afternoon'] = X['Hour_of_Day'].apply(lambda x: 1 if 12 <= x <= 17 else 0)\n",
        "X['Evening'] = X['Hour_of_Day'].apply(lambda x: 1 if 18 <= x <= 23 else 0)\n",
        "X['Night'] = X['Hour_of_Day'].apply(lambda x: 1 if 0 <= x <= 4 else 0)\n",
        "\n",
        "# Adding a 7-day rolling average for key numerical features to capture general trends\n",
        "# Assuming CallDurationRatio and BarrierOpenedCount are present in the dataset\n",
        "X['RollingMean_CallDurationRatio'] = X['CallDurationRatio'].rolling(window=7, min_periods=1).mean()\n",
        "X['RollingMean_BarrierOpenedCount'] = X['BarrierOpenedCount'].rolling(window=7, min_periods=1).mean()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
